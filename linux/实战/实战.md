# Linux 入门必看：如何60秒内分析Linux性能

https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55

https://mp.weixin.qq.com/s/HvADkICPYflS2VTuSB16rg

http://www.brendangregg.com/tsamethod.html

# 排查步骤

vmstat     -->     top   -->  pidstat
        通过vmstat查看整体CPU, 内存, io,使用情况，top查看CPU占用高的几个进程，pidstat查看这几个进程对应的线程。
        监控CPU性能包括以下几个部分：
       a. 检查CPU的run queue，每个CPU的run queue最好不要超过3个进程。
       b. 确定CPU利用率在usr/sys = 65% / 35% ~ 70% / 30%之间。
       c. 当CPU的处理时间更多的是在system空间，说明已经超负荷。
       d. 当I/O增多时，CPU密集型的应用将受到影响。
       e. 当CPU的IOWait占用比较大的比例时，说明IO出现异常。


# 碰到上下文切换次数过多

可以借助 vmstat  、  pidstat 和 /proc/interrupts 等工具，来辅助排查性能问题的根源。

## Kworker占用了CPU, 同时大量的自愿切换

排查过程:

- top 发现僵尸进程和kworker

- `kill -9` 发现杀不死

- ps 发现是docker中的java,  再杀父进程,不OK, 

  ![image-20200609212612136](image-20200609212612136.png)

  根据进程号`ps -ef | grep PID`找到docker ID

  `docker stop Conatiner ID`关不掉

  思考是不是因为kwoker

- `pidstat -w` 发现上下文切换很高

- `perf record -ag -- sleep 15; perf report` 没有发现什么有价值且能看懂的信息

- google

  - [Kworker, what is it and why is it hogging so much CPU?](https://askubuntu.com/questions/33640/kworker-what-is-it-and-why-is-it-hogging-so-much-cpu)
  - 第一个答案介绍kworker, 看到了重启大法, 稍后再试
  - 第二个答案, 给出了他的解决方案,实验无果
  - 第三个答案, 说因为NFS挂载的原因, -->想到自己的挂载卷
  - `df -Th` 验证处于 夯住的状态
  - `cat /etc/mtab` 结合前面的容器名字 找到挂载的卷
  
- 使用 umount -f PATH 卸载

  ```
  ~ umount -f /webserver/page
  ```
强制卸载, 如果执行此命令后, 还是提示磁盘"busily" 就使用以下命令卸载

  使用 umount -l PATH 卸载

  ```
 ~ umount -l /webserver/page
  ```
  
- 僵尸进程没有了

- kworker 依旧很高

- 重启

- 事后继续查看perf 发现

  <img src="image-20200609232932528.png" alt="image-20200609232932528" style="zoom:67%;" />

  <img src="image-20200609233221022.png" alt="image-20200609233221022" style="zoom:67%;" />
  
  以目前的经验分析, 的确和nfs有关,因为nfs依赖于rpc, 强制卸载盘后依旧无法释放连接

# CPU使用率过高怎么办

pref

# 进程的 PID 在变，这说明什么呢？

- 要么是这些进程在不停地重启，要么就是全新的进程，这无非也就两个原因：
  - 进程在不停地崩溃重启，比如因为段错误、配置错误等等，这时，进程在退出后可能又被监控系统自动重启了。
  - 这些进程都是短时进程，也就是在其他应用内部通过 exec 调用的外面命令。这些命令一般都只运行很短的时间就会结束，你很难用 top 这种间隔时间比较长的工具发现

# 碰见iowait升高怎么办

- 碰到 iowait 升高时，需要先用 dstat、pidstat 等工具，确认是不是磁盘 I/O 的问题，然后再找是哪些进程导致了 I/O。
- 确定是不是内存满了， swap问题
- 等待 I/O 的进程一般是不可中断状态，所以用 ps 命令找到的 D 状态（即不可中断状态）的进程，多为可疑进程。但这个案例中，在 I/O 操作后，进程又变成了僵尸进程，所以不能用 strace 直接分析这个进程的系统调用。这种情况下，我们用了 perf 工具，来分析系统的 CPU 时钟事件，最终发现是直接 I/O 导致的问题。这时，再检查源码中对应位置的问题，就很轻松了。而僵尸进程的问题相对容易排查，使用 pstree 找出父进程后，去查看父进程的代码，检查 wait() / waitpid() 的调用，或是 SIGCHLD 信号处理函数的注册就行了。

# CPU 的性能分析

1. top 看一眼load average 

2. top + 1看**cpu使用率**

3. us 高 说明 用户程序比较繁忙

   sy 高  说明 内核比较繁忙   --> pidstat -wut 抓进程

   io 高 **通常**说明系统与硬件设备的 I/O 交互时间比较长。  pidstat -d 抓进程

   si/hi 软中断 硬中断, --> watch -d cat /proc/softirqs 

4.  去看 **上下文**

5. cpu **缓存的命中率**

   ![img](../image/7a445960a4bc0a58a02e1bc75648aa17.png)

# 内存的性能分析

1. 先用 free 和 top，查看系统整体的内存使用情况。
2. vmstat 和 pidstat，查看一段时间的趋势，从而判断出内存问题的类型。
3. 最后进行详细分析，比如内存分配分析、缓存 / 缓冲区分析、具体进程的内存使用分析等。

![img](../image/memory_experience.png)

# io性能分析

![img](../image/io.png)

![img](../image/1802a35475ee2755fb45aec55ed2d98a.png)